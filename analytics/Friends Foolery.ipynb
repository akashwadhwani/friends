{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ef4c7d53-526d-45a7-9a65-be5f6ed55ad8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watson_developer_cloud in /Users/Timothy/anaconda/lib/python3.5/site-packages\r\n",
      "Requirement already satisfied: pysolr<4.0,>=3.3 in /Users/Timothy/anaconda/lib/python3.5/site-packages (from watson_developer_cloud)\r\n",
      "Requirement already satisfied: requests<3.0,>=2.0 in /Users/Timothy/anaconda/lib/python3.5/site-packages (from watson_developer_cloud)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install watson_developer_cloud\n",
    "from pandas.io.json import json_normalize\n",
    "from watson_developer_cloud import  PersonalityInsightsV3\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, ngrams\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import string\n",
    "import random\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2d754860-3326-4d3d-b1ae-9a89a3be8ba6"
    }
   },
   "outputs": [],
   "source": [
    "characters = [\"RACHEL\", \"ROSS\", \"MONICA\", \"CHANDLER\", \"JOEY\", \"PHOEBE\"]\n",
    "\n",
    "personal_stopwords = [\"yeah\", \"okay\", \"gon\", \"na\", \"gonna\",\n",
    "                      \"oh\", \"hey\", \"well\", \"im\", \"right\",\n",
    "                      \"like\", \"its\"]\n",
    "personal_stopwords = [\"gon\", \"na\"]\n",
    "\n",
    "stop = stopwords.words('english') + personal_stopwords\n",
    "no_punc = str.maketrans(string.punctuation, len(string.punctuation)*\" \")\n",
    "printable = set(string.printable)\n",
    "\n",
    "def clean_line(text):\n",
    "    return ''.join(list((filter(lambda x: x in printable, str(text)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b6cb92ce-c114-4602-9446-e8c4a84a7a4d"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001.csv 1002.csv 1003.csv 1004.csv 1005.csv 1006.csv 1007.csv 1008.csv 1009.csv 101.csv  1010.csv \n",
      "1011.csv 1012.csv 1013.csv 1014.csv 1015.csv 1016.csv 1017.csv 102.csv  103.csv  104.csv  105.csv  \n",
      "106.csv  107.csv  108.csv  109.csv  110.csv  111.csv  112.csv  113.csv  114.csv  115.csv  116.csv  \n",
      "117.csv  118.csv  119.csv  120.csv  121.csv  122.csv  123.csv  124.csv  201.csv  202.csv  203.csv  \n",
      "204.csv  205.csv  206.csv  207.csv  208.csv  209.csv  210.csv  211.csv  212.csv  214.csv  215.csv  \n",
      "216.csv  217.csv  218.csv  219.csv  220.csv  221.csv  222.csv  223.csv  224.csv  301.csv  302.csv  \n",
      "303.csv  304.csv  305.csv  306.csv  307.csv  308.csv  309.csv  310.csv  311.csv  312.csv  313.csv  \n",
      "314.csv  315.csv  316.csv  317.csv  318.csv  319.csv  320.csv  321.csv  322.csv  323.csv  324.csv  \n",
      "325.csv  401.csv  402.csv  403.csv  404.csv  405.csv  406.csv  407.csv  408.csv  409.csv  410.csv  \n",
      "411.csv  412.csv  413.csv  414.csv  415.csv  416.csv  417.csv  418.csv  419.csv  420.csv  421.csv  \n",
      "422.csv  423.csv  501.csv  502.csv  503.csv  504.csv  505.csv  506.csv  507.csv  508.csv  509.csv  \n",
      "510.csv  511.csv  512.csv  513.csv  514.csv  515.csv  516.csv  517.csv  518.csv  519.csv  520.csv  \n",
      "521.csv  522.csv  523.csv  601.csv  602.csv  603.csv  604.csv  605.csv  606.csv  607.csv  608.csv  \n",
      "609.csv  610.csv  611.csv  612.csv  613.csv  614.csv  615.csv  617.csv  618.csv  619.csv  620.csv  \n",
      "621.csv  622.csv  623.csv  624.csv  701.csv  702.csv  703.csv  704.csv  705.csv  706.csv  707.csv  \n",
      "708.csv  709.csv  710.csv  711.csv  712.csv  713.csv  714.csv  715.csv  716.csv  717.csv  718.csv  \n",
      "719.csv  720.csv  721.csv  722.csv  723.csv  801.csv  802.csv  803.csv  804.csv  805.csv  806.csv  \n",
      "807.csv  808.csv  809.csv  810.csv  811.csv  812.csv  813.csv  814.csv  815.csv  816.csv  817.csv  \n",
      "818.csv  819.csv  820.csv  821.csv  822.csv  823.csv  901.csv  902.csv  903.csv  904.csv  905.csv  \n",
      "906.csv  907.csv  908.csv  909.csv  910.csv  911.csv  912.csv  913.csv  914.csv  915.csv  916.csv  \n",
      "917.csv  918.csv  919.csv  920.csv  921.csv  922.csv  923.csv  924.csv  "
     ]
    }
   ],
   "source": [
    "term_frequencies = { character: {} for character in characters }\n",
    "all_text = { character: [] for character in characters }\n",
    "word_count = { character: 0 for character in characters }\n",
    "\n",
    "print_count = 0\n",
    "\n",
    "for transcript in listdir(\"./data/transcripts\")[:]:\n",
    "    df = pd.read_csv(\"./data/transcripts/{}\".format(transcript))\n",
    "    df.dropna()\n",
    "    print(transcript.ljust(8), end=\" \")\n",
    "    \n",
    "    if print_count == 10:\n",
    "        print()\n",
    "        print_count = 0\n",
    "    else:\n",
    "        print_count += 1\n",
    "        \n",
    "    for character in characters:\n",
    "        for line in df[df[\"Speaker\"].str.contains(character)][\"Line\"]:\n",
    "            char_term_frequencies = term_frequencies[character]\n",
    "            line = clean_line(line)\n",
    "            all_text[character].append(line)\n",
    "            tokens = word_tokenize(line.translate(no_punc))\n",
    "            for word in tokens:\n",
    "                word_count[character] += 1\n",
    "                if word in char_term_frequencies:\n",
    "                    char_term_frequencies[word] += 1\n",
    "                else:\n",
    "                    char_term_frequencies[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2138f3bd-6200-47e0-baa8-718019c3549d"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RACHEL said 102239 words (18.18%)\n",
      "RACHEL's top 10 words were\n",
      "oh was said 1936 times\n",
      "know was said 1051 times\n",
      "okay was said 852 times\n",
      "well was said 825 times\n",
      "yeah was said 804 times\n",
      "right was said 556 times\n",
      "im was said 553 times\n",
      "ross was said 523 times\n",
      "god was said 484 times\n",
      "really was said 455 times\n",
      "\n",
      "ROSS said 100824 words (17.92%)\n",
      "ROSS's top 10 words were\n",
      "oh was said 980 times\n",
      "know was said 886 times\n",
      "yeah was said 856 times\n",
      "okay was said 844 times\n",
      "hey was said 778 times\n",
      "uh was said 747 times\n",
      "well was said 603 times\n",
      "right was said 514 times\n",
      "im was said 506 times\n",
      "like was said 446 times\n",
      "\n",
      "MONICA said 88174 words (15.68%)\n",
      "MONICA's top 10 words were\n",
      "oh was said 978 times\n",
      "okay was said 675 times\n",
      "know was said 663 times\n",
      "well was said 453 times\n",
      "right was said 441 times\n",
      "hey was said 431 times\n",
      "yeah was said 417 times\n",
      "get was said 406 times\n",
      "go was said 406 times\n",
      "really was said 394 times\n",
      "\n",
      "CHANDLER said 91935 words (16.34%)\n",
      "CHANDLER's top 10 words were\n",
      "oh was said 811 times\n",
      "know was said 802 times\n",
      "okay was said 713 times\n",
      "well was said 636 times\n",
      "yeah was said 561 times\n",
      "hey was said 548 times\n",
      "right was said 483 times\n",
      "get was said 439 times\n",
      "like was said 412 times\n",
      "im was said 371 times\n",
      "\n",
      "JOEY said 92766 words (16.49%)\n",
      "JOEY's top 10 words were\n",
      "hey was said 1132 times\n",
      "yeah was said 994 times\n",
      "oh was said 905 times\n",
      "know was said 805 times\n",
      "right was said 670 times\n",
      "okay was said 561 times\n",
      "like was said 559 times\n",
      "well was said 527 times\n",
      "uh was said 502 times\n",
      "got was said 487 times\n",
      "\n",
      "PHOEBE said 86545 words (15.39%)\n",
      "PHOEBE's top 10 words were\n",
      "oh was said 1468 times\n",
      "know was said 1011 times\n",
      "okay was said 823 times\n",
      "yeah was said 805 times\n",
      "well was said 583 times\n",
      "like was said 535 times\n",
      "hey was said 472 times\n",
      "right was said 413 times\n",
      "really was said 382 times\n",
      "im was said 376 times\n",
      "\n",
      "283\n"
     ]
    }
   ],
   "source": [
    "common_words = []\n",
    "with open('./data/word_count.json', 'w') as count_json:\n",
    "    count_json.write('[\\n')\n",
    "    \n",
    "    for character in characters:\n",
    "        char_term_freqs = term_frequencies[character]\n",
    "        percent_words = round((word_count[character]/sum(word_count.values()))*100, 2)\n",
    "        print(\"{} said {} words ({}%)\".format(character,\n",
    "                                              word_count[character],\n",
    "                                              percent_words))\n",
    "        print(\"{}'s top 10 words were\".format(character))\n",
    "        \n",
    "        sorted_words = sorted(char_term_freqs, key=char_term_freqs.get, reverse=True)\n",
    "        ith_word = 0\n",
    "        to_print = True\n",
    "        char_output = {\n",
    "            \"name\": character,\n",
    "            \"percent\": percent_words,\n",
    "            \"uniqueWords\": len(char_term_freqs.keys()),\n",
    "            \"words\": {}\n",
    "        }\n",
    "            \n",
    "        for word in sorted_words:\n",
    "            if word not in stop:\n",
    "                if (to_print):\n",
    "                    print(\"{} was said {} times\".format(word, char_term_freqs[word]))\n",
    "                common_words.append(word)\n",
    "                char_output[\"words\"][word] = char_term_freqs[word]\n",
    "                ith_word += 1\n",
    "            if ith_word == 10:\n",
    "                to_print = False\n",
    "            if ith_word == 250:\n",
    "                break\n",
    "                \n",
    "        json.dump(char_output, count_json, indent=4)\n",
    "        count_json.write(',\\n')\n",
    "        print()\n",
    "    common = set([x for x in common_words if common_words.count(x) > 1])\n",
    "    common_occurences = {}\n",
    "    for word in common:\n",
    "        occurences = 0\n",
    "        for character in characters:\n",
    "            occurences += term_frequencies[character][word]\n",
    "        common_occurences[word] = occurences\n",
    "    common_output = {\n",
    "        \"name\": \"COMMON\",\n",
    "        \"percent\": 100,\n",
    "        \"uniqueWords\": 100,\n",
    "        \"words\": common_occurences\n",
    "    }\n",
    "    print(len(common_output[\"words\"]))\n",
    "    json.dump(common_output, count_json, indent=4)\n",
    "    count_json.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f1cadb8c-8adb-4ad5-8988-c77d88afd68d"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RACHEL speaks 0.107% less/more than than the median\n",
      "ROSS speaks 0.092% less/more than than the median\n",
      "MONICA speaks -0.045% less/more than than the median\n",
      "CHANDLER speaks -0.004% less/more than than the median\n",
      "JOEY speaks 0.004% less/more than than the median\n",
      "PHOEBE speaks -0.063% less/more than than the median\n",
      "\n",
      "RACHEL says 9434 lines\n",
      "ROSS says 9246 lines\n",
      "MONICA says 8618 lines\n",
      "CHANDLER says 8621 lines\n",
      "JOEY says 8452 lines\n",
      "PHOEBE says 7667 lines\n"
     ]
    }
   ],
   "source": [
    "freqs = [value for value in word_count.values()]\n",
    "median = statistics.median(freqs)\n",
    "with open('./data/most-talkative.json', 'w') as most_talkative:\n",
    "    most_talkative.write(\"[\\n\")\n",
    "    median_json = {\n",
    "        \"name\": \"median\",\n",
    "        \"words\": median\n",
    "    }\n",
    "    json.dump(median_json, most_talkative, indent=4)\n",
    "    most_talkative.write(\",\\n\")\n",
    "    for character in characters:\n",
    "        difference = round((word_count[character]/median) - 1, 3)\n",
    "        print(\"{} speaks {}% less/more than than the median\".format(character, difference))\n",
    "        char_output = {\n",
    "            \"name\": character,\n",
    "            \"words\": word_count[character],\n",
    "            \"frequency\": difference\n",
    "        }\n",
    "        json.dump(char_output, most_talkative, indent=4)\n",
    "        if character is not characters[-1]:\n",
    "            most_talkative.write(\",\\n\")\n",
    "    most_talkative.write(\"]\")\n",
    "        \n",
    "    \n",
    "print()\n",
    "\n",
    "for character in characters:\n",
    "    print(\"{} says {} lines\".format(character, len(all_text[character])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "4a4a90c9-f36a-4e35-947e-820d74ad2f6b"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel   -> Ross     523 times (37%)\n",
      "Rachel   -> Monica   215 times (15%)\n",
      "Rachel   -> Chandler 149 times (10%)\n",
      "Rachel   -> Joey     333 times (23%)\n",
      "Rachel   -> Phoebe   205 times (14%)\n",
      "\n",
      "Ross     -> Rachel   235 times (30%)\n",
      "Ross     -> Monica   135 times (17%)\n",
      "Ross     -> Chandler 135 times (17%)\n",
      "Ross     -> Joey     175 times (22%)\n",
      "Ross     -> Phoebe   99  times (13%)\n",
      "\n",
      "Monica   -> Rachel   188 times (16%)\n",
      "Monica   -> Ross     205 times (18%)\n",
      "Monica   -> Chandler 283 times (24%)\n",
      "Monica   -> Joey     229 times (20%)\n",
      "Monica   -> Phoebe   259 times (22%)\n",
      "\n",
      "Chandler -> Rachel   66  times (9%)\n",
      "Chandler -> Ross     178 times (24%)\n",
      "Chandler -> Monica   197 times (27%)\n",
      "Chandler -> Joey     220 times (30%)\n",
      "Chandler -> Phoebe   70  times (10%)\n",
      "\n",
      "Joey     -> Rachel   118 times (14%)\n",
      "Joey     -> Ross     337 times (39%)\n",
      "Joey     -> Monica   122 times (14%)\n",
      "Joey     -> Chandler 213 times (25%)\n",
      "Joey     -> Phoebe   78  times (9%)\n",
      "\n",
      "Phoebe   -> Rachel   149 times (19%)\n",
      "Phoebe   -> Ross     184 times (23%)\n",
      "Phoebe   -> Monica   168 times (21%)\n",
      "Phoebe   -> Chandler 143 times (18%)\n",
      "Phoebe   -> Joey     155 times (19%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = {}\n",
    "\n",
    "for character in characters:\n",
    "    network[character] = {\n",
    "        \"Total\": 0\n",
    "    }\n",
    "for source in characters:\n",
    "    for dest in characters:\n",
    "        if source is dest:\n",
    "            continue\n",
    "        dest = dest.lower()\n",
    "        mentions = term_frequencies[source][dest]\n",
    "        if dest is \"phoebe\":\n",
    "            mentions += term_frequencies[source][\"pheebs\"]\n",
    "        if dest is \"rachel\":\n",
    "            mentions += term_frequencies[source][\"rach\"]\n",
    "        if dest is \"monica\":\n",
    "            #\"c'mon is cheaper to parse here than it is in building the term_frequencies\n",
    "            mentions += term_frequencies[source][\"mon\"] - term_frequencies[source][\"c\"] \n",
    "        if dest is \"joey\":\n",
    "            mentions += term_frequencies[source][\"joe\"]\n",
    "        network[source][dest.upper()] = mentions\n",
    "        network[source][\"Total\"] += mentions\n",
    "    \n",
    "    for dest in characters:\n",
    "        if source is dest:\n",
    "            continue\n",
    "        mentions = network[source][dest]\n",
    "        percent_mentions = round((mentions/network[source][\"Total\"])*100)\n",
    "        print(\"{} -> {} {} times ({}%)\".format(source.title().ljust(8),\n",
    "                                               dest.title().ljust(8),\n",
    "                                               str(mentions).ljust(3),\n",
    "                                               percent_mentions))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes = list(map(lambda name: {\"name\": name}, characters))\n",
    "links = []\n",
    "\n",
    "for source in characters:\n",
    "    for dest in characters:\n",
    "        if source is dest:\n",
    "            continue\n",
    "        mentions = network[source][dest]\n",
    "        links.append({\n",
    "            \"source\": source,\n",
    "            \"target\": dest,\n",
    "            \"value\": mentions,\n",
    "            \"percent\": round((mentions/network[source][\"Total\"])*100)\n",
    "        })\n",
    "graph = [nodes, links]\n",
    "with open('./data/network.json', 'w') as outfile:\n",
    "    json.dump(graph, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "personality_insights = PersonalityInsightsV3(\n",
    "    username=PERSONALITY_USERNAME,\n",
    "    password=PERSONALITY_PASSWORD,\n",
    "    version='2016-10-20')\n",
    "\n",
    "for character in characters:\n",
    "    char_all_text = all_text[character]\n",
    "    random.shuffle(char_all_text)\n",
    "    joined_text = ' '.join(char_all_text)\n",
    "    json_response = personality_insights.profile(joined_text)\n",
    "    with open('./data/personality-{}.json'.format(character), 'w') as outfile:\n",
    "        json.dump(json_response, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiery\n",
      "Rachel: 0.11\n",
      "Ross: 0.09\n",
      "Monica: 0.1\n",
      "Chandler: 0.1\n",
      "Joey: 0.07\n",
      "Phoebe: 0.11\n",
      "\n",
      "Authority-challenging\n",
      "Rachel: 0.49\n",
      "Ross: 0.54\n",
      "Monica: 0.45\n",
      "Chandler: 0.55\n",
      "Joey: 0.52\n",
      "Phoebe: 0.47\n",
      "\n",
      "Conscientiousness\n",
      "Rachel: 0.81\n",
      "Ross: 0.79\n",
      "Monica: 0.86\n",
      "Chandler: 0.79\n",
      "Joey: 0.82\n",
      "Phoebe: 0.81\n",
      "\n",
      "Activity level\n",
      "Rachel: 0.08\n",
      "Ross: 0.09\n",
      "Monica: 0.11\n",
      "Chandler: 0.11\n",
      "Joey: 0.1\n",
      "Phoebe: 0.07\n",
      "\n",
      "Uncompromising\n",
      "Rachel: 1.0\n",
      "Ross: 1.0\n",
      "Monica: 1.0\n",
      "Chandler: 0.99\n",
      "Joey: 0.99\n",
      "Phoebe: 0.99\n",
      "\n",
      "Orderliness\n",
      "Rachel: 0.32\n",
      "Ross: 0.37\n",
      "Monica: 0.4\n",
      "Chandler: 0.42\n",
      "Joey: 0.54\n",
      "Phoebe: 0.45\n",
      "\n",
      "Gregariousness\n",
      "Rachel: 0.28\n",
      "Ross: 0.27\n",
      "Monica: 0.4\n",
      "Chandler: 0.33\n",
      "Joey: 0.43\n",
      "Phoebe: 0.27\n",
      "\n",
      "Imagination\n",
      "Rachel: 0.02\n",
      "Ross: 0.03\n",
      "Monica: 0.03\n",
      "Chandler: 0.04\n",
      "Joey: 0.04\n",
      "Phoebe: 0.04\n",
      "\n",
      "Immoderation\n",
      "Rachel: 0.36\n",
      "Ross: 0.33\n",
      "Monica: 0.42\n",
      "Chandler: 0.39\n",
      "Joey: 0.41\n",
      "Phoebe: 0.32\n",
      "\n",
      "Intellect\n",
      "Rachel: 0.53\n",
      "Ross: 0.72\n",
      "Monica: 0.64\n",
      "Chandler: 0.69\n",
      "Joey: 0.54\n",
      "Phoebe: 0.57\n",
      "\n",
      "Modesty\n",
      "Rachel: 1.0\n",
      "Ross: 1.0\n",
      "Monica: 1.0\n",
      "Chandler: 0.99\n",
      "Joey: 0.99\n",
      "Phoebe: 1.0\n",
      "\n",
      "Dutifulness\n",
      "Rachel: 0.72\n",
      "Ross: 0.7\n",
      "Monica: 0.73\n",
      "Chandler: 0.65\n",
      "Joey: 0.57\n",
      "Phoebe: 0.65\n",
      "\n",
      "Extraversion\n",
      "Rachel: 0.46\n",
      "Ross: 0.39\n",
      "Monica: 0.48\n",
      "Chandler: 0.41\n",
      "Joey: 0.51\n",
      "Phoebe: 0.4\n",
      "\n",
      "Emotional range\n",
      "Rachel: 0.63\n",
      "Ross: 0.68\n",
      "Monica: 0.71\n",
      "Chandler: 0.72\n",
      "Joey: 0.75\n",
      "Phoebe: 0.68\n",
      "\n",
      "Prone to worry\n",
      "Rachel: 0.19\n",
      "Ross: 0.14\n",
      "Monica: 0.13\n",
      "Chandler: 0.12\n",
      "Joey: 0.08\n",
      "Phoebe: 0.15\n",
      "\n",
      "Melancholy\n",
      "Rachel: 0.45\n",
      "Ross: 0.38\n",
      "Monica: 0.38\n",
      "Chandler: 0.36\n",
      "Joey: 0.28\n",
      "Phoebe: 0.4\n",
      "\n",
      "Excitement-seeking\n",
      "Rachel: 0.09\n",
      "Ross: 0.07\n",
      "Monica: 0.1\n",
      "Chandler: 0.1\n",
      "Joey: 0.11\n",
      "Phoebe: 0.09\n",
      "\n",
      "Trust\n",
      "Rachel: 0.17\n",
      "Ross: 0.18\n",
      "Monica: 0.18\n",
      "Chandler: 0.16\n",
      "Joey: 0.17\n",
      "Phoebe: 0.16\n",
      "\n",
      "Cautiousness\n",
      "Rachel: 0.92\n",
      "Ross: 0.92\n",
      "Monica: 0.91\n",
      "Chandler: 0.9\n",
      "Joey: 0.9\n",
      "Phoebe: 0.91\n",
      "\n",
      "Altruism\n",
      "Rachel: 0.64\n",
      "Ross: 0.6\n",
      "Monica: 0.68\n",
      "Chandler: 0.6\n",
      "Joey: 0.47\n",
      "Phoebe: 0.57\n",
      "\n",
      "Self-consciousness\n",
      "Rachel: 0.51\n",
      "Ross: 0.5\n",
      "Monica: 0.4\n",
      "Chandler: 0.43\n",
      "Joey: 0.37\n",
      "Phoebe: 0.48\n",
      "\n",
      "Outgoing\n",
      "Rachel: 0.3\n",
      "Ross: 0.33\n",
      "Monica: 0.4\n",
      "Chandler: 0.37\n",
      "Joey: 0.44\n",
      "Phoebe: 0.3\n",
      "\n",
      "Assertiveness\n",
      "Rachel: 0.07\n",
      "Ross: 0.1\n",
      "Monica: 0.18\n",
      "Chandler: 0.18\n",
      "Joey: 0.15\n",
      "Phoebe: 0.08\n",
      "\n",
      "Cheerfulness\n",
      "Rachel: 0.79\n",
      "Ross: 0.62\n",
      "Monica: 0.77\n",
      "Chandler: 0.72\n",
      "Joey: 0.74\n",
      "Phoebe: 0.76\n",
      "\n",
      "Susceptible to stress\n",
      "Rachel: 0.59\n",
      "Ross: 0.52\n",
      "Monica: 0.65\n",
      "Chandler: 0.43\n",
      "Joey: 0.41\n",
      "Phoebe: 0.53\n",
      "\n",
      "Openness\n",
      "Rachel: 0.55\n",
      "Ross: 0.63\n",
      "Monica: 0.6\n",
      "Chandler: 0.67\n",
      "Joey: 0.66\n",
      "Phoebe: 0.63\n",
      "\n",
      "Achievement striving\n",
      "Rachel: 0.33\n",
      "Ross: 0.33\n",
      "Monica: 0.39\n",
      "Chandler: 0.34\n",
      "Joey: 0.33\n",
      "Phoebe: 0.3\n",
      "\n",
      "Cooperation\n",
      "Rachel: 0.97\n",
      "Ross: 0.96\n",
      "Monica: 0.94\n",
      "Chandler: 0.93\n",
      "Joey: 0.94\n",
      "Phoebe: 0.96\n",
      "\n",
      "Agreeableness\n",
      "Rachel: 0.99\n",
      "Ross: 0.99\n",
      "Monica: 0.99\n",
      "Chandler: 0.98\n",
      "Joey: 0.98\n",
      "Phoebe: 0.99\n",
      "\n",
      "Self-discipline\n",
      "Rachel: 0.61\n",
      "Ross: 0.6\n",
      "Monica: 0.7\n",
      "Chandler: 0.62\n",
      "Joey: 0.66\n",
      "Phoebe: 0.6\n",
      "\n",
      "Emotionality\n",
      "Rachel: 0.65\n",
      "Ross: 0.58\n",
      "Monica: 0.64\n",
      "Chandler: 0.58\n",
      "Joey: 0.49\n",
      "Phoebe: 0.56\n",
      "\n",
      "Artistic interests\n",
      "Rachel: 0.64\n",
      "Ross: 0.71\n",
      "Monica: 0.65\n",
      "Chandler: 0.7\n",
      "Joey: 0.7\n",
      "Phoebe: 0.77\n",
      "\n",
      "Adventurousness\n",
      "Rachel: 0.71\n",
      "Ross: 0.76\n",
      "Monica: 0.73\n",
      "Chandler: 0.78\n",
      "Joey: 0.83\n",
      "Phoebe: 0.72\n",
      "\n",
      "Sympathy\n",
      "Rachel: 0.78\n",
      "Ross: 0.77\n",
      "Monica: 0.77\n",
      "Chandler: 0.76\n",
      "Joey: 0.61\n",
      "Phoebe: 0.74\n",
      "\n",
      "Self-efficacy\n",
      "Rachel: 0.01\n",
      "Ross: 0.01\n",
      "Monica: 0.03\n",
      "Chandler: 0.03\n",
      "Joey: 0.02\n",
      "Phoebe: 0.01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "personalities = {character: {} for character in characters}\n",
    "\n",
    "for character in characters:\n",
    "    with open('./data/personality-{}.json'.format(character), 'r') as data:\n",
    "        personality = json.load(data)\n",
    "        for trait in personality[\"personality\"]:\n",
    "            personalities[character][trait[\"name\"]] = trait[\"percentile\"]\n",
    "            for child in trait[\"children\"]:\n",
    "                personalities[character][child[\"name\"]] = child[\"percentile\"]\n",
    "            \n",
    "traits = personalities[list(personalities.keys())[0]].keys()\n",
    "\n",
    "for trait in traits:\n",
    "    print(trait)\n",
    "    for character in characters:\n",
    "        print(\"{}: {}\".format(character.title(), round(personalities[character][trait],2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel Ross Monica Chandler Joey Phoebe "
     ]
    }
   ],
   "source": [
    "sentiments = {}\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_sum = 0\n",
    "sentiment_count = 0\n",
    "for character in characters:\n",
    "    char_all_text = all_text[character]\n",
    "    sentiments[character] = {\n",
    "        \"positive\": 0,\n",
    "        \"neutral\": 0,\n",
    "        \"negative\": 0,\n",
    "        \"total\": len(char_all_text)\n",
    "    }\n",
    "    print(\"{} \".format(character.title()), end=\"\")\n",
    "    for line in char_all_text:\n",
    "        sentiment = sid.polarity_scores(line)[\"compound\"]\n",
    "        sentiment_sum += sentiment\n",
    "        sentiment_count += 1\n",
    "        if sentiment < -0.5:\n",
    "            key = \"negative\"\n",
    "        elif sentiment < 0.5:\n",
    "            key = \"neutral\"\n",
    "        else:\n",
    "            key = \"positive\"\n",
    "        sentiments[character][key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 0.13\n",
      "\n",
      "Rachel\n",
      "Positive: 1741 (18.45%)\n",
      "Neutral:  7198 (76.3%)\n",
      "Negative: 495  (5.25%)\n",
      "\n",
      "Ross\n",
      "Positive: 1559 (16.86%)\n",
      "Neutral:  7205 (77.93%)\n",
      "Negative: 482  (5.21%)\n",
      "\n",
      "Monica\n",
      "Positive: 1347 (15.63%)\n",
      "Neutral:  6804 (78.95%)\n",
      "Negative: 467  (5.42%)\n",
      "\n",
      "Chandler\n",
      "Positive: 1381 (16.02%)\n",
      "Neutral:  6757 (78.38%)\n",
      "Negative: 483  (5.6%)\n",
      "\n",
      "Joey\n",
      "Positive: 1443 (17.07%)\n",
      "Neutral:  6534 (77.31%)\n",
      "Negative: 475  (5.62%)\n",
      "\n",
      "Phoebe\n",
      "Positive: 1501 (19.58%)\n",
      "Neutral:  5688 (74.19%)\n",
      "Negative: 478  (6.23%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Average: {}\\n\".format(round(sentiment_sum/sentiment_count, 2)))\n",
    "\n",
    "for character in characters:\n",
    "    sent = sentiments[character]\n",
    "    positive = round((sent[\"positive\"]/sent[\"total\"])*100, 2)\n",
    "    neutral  = round((sent[\"neutral\"]/sent[\"total\"]) *100, 2)\n",
    "    negative = round((sent[\"negative\"]/sent[\"total\"])*100, 2)\n",
    "    print(character.title())\n",
    "    print(\"Positive: {} ({}%)\".format(sent[\"positive\"], positive))\n",
    "    print(\"Neutral:  {} ({}%)\".format(sent[\"neutral\"], neutral))\n",
    "    print(\"Negative: {}  ({}%)\\n\".format(sent[\"negative\"], negative))\n",
    "    \n",
    "with open('./data/sentiments.json', 'w') as outfile:\n",
    "        json.dump(sentiments, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross-filterable word clouds\n",
    "# force-directed bezier graphs of the social network\n",
    "# bubble chart of who talks the most, normalized\n",
    "# sentiment grouped bar chart w/ error analysis\n",
    "# tri-gram map\n",
    "# personality radar charts with big 5 breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rachel\n",
      "\"oh my god\" was said 256 times\n",
      "\"i dont know\" was said 138 times\n",
      "\"no no no\" was said 135 times\n",
      "\"im gonna\" was said 120 times\n",
      "\"you know what\" was said 100 times\n",
      "\n",
      "Ross\n",
      "\"i dont know\" was said 122 times\n",
      "\"oh my god\" was said 114 times\n",
      "\"im gonna\" was said 102 times\n",
      "\"no no no\" was said 102 times\n",
      "\"gonna be\" was said 88 times\n",
      "\n",
      "Monica\n",
      "\"oh my god\" was said 232 times\n",
      "\"what are you\" was said 98 times\n",
      "\"im gonna\" was said 89 times\n",
      "\"gonna be\" was said 88 times\n",
      "\"i cant believe\" was said 72 times\n",
      "\n",
      "Chandler\n",
      "\"no no no\" was said 199 times\n",
      "\"im gonna\" was said 92 times\n",
      "\"i dont know\" was said 85 times\n",
      "\"gonna be\" was said 82 times\n",
      "\"what are you\" was said 72 times\n",
      "\n",
      "Joey\n",
      "\"no no no\" was said 188 times\n",
      "\"i dont know\" was said 136 times\n",
      "\"im gonna\" was said 101 times\n",
      "\"gonna be\" was said 82 times\n",
      "\"what are you\" was said 67 times\n",
      "\n",
      "Phoebe\n",
      "\"oh my god\" was said 173 times\n",
      "\"im gonna\" was said 92 times\n",
      "\"i dont know\" was said 87 times\n",
      "\"no no no\" was said 68 times\n",
      "\"gonna be\" was said 58 times\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fix_unicode_gaps(text):\n",
    "    text = text.replace(\"don t\", \"dont\")\n",
    "    text = text.replace(\"you re\", \"youre\")\n",
    "    text = text.replace(\"i m\", \"im\")\n",
    "    text = text.replace(\"i ll\", \"ill\")\n",
    "    text = text.replace(\"we re\", \"were\")\n",
    "    text = text.replace(\"it s\", \"its\")\n",
    "    text = text.replace(\"can t\", \"cant\")\n",
    "    text = text.replace(\"won t\", \"wont\")\n",
    "    text = text.replace(\"that s\", \"thats\")\n",
    "    text = text.replace(\"didn t\", \"didnt\")\n",
    "    text = text.replace(\"y know\", \"yknow\")\n",
    "    return text\n",
    "\n",
    "def repair_nltk_contractions(text):\n",
    "    text = text.replace(\"gon na\", \"gonna\")\n",
    "    text = text.replace(\"wan na\", \"wanna\")\n",
    "    text = text.replace(\"got ta\", \"gotta\")\n",
    "    text = text.replace(\"they ll\", \"theyll\")\n",
    "    text = text.replace(\"smell ly\", \"smelly\")\n",
    "    return text\n",
    "\n",
    "gram_size = 3\n",
    "output_dict = {}\n",
    "for character in characters:\n",
    "    ngram_list = []\n",
    "    for line in all_text[character]:\n",
    "        tokens = word_tokenize(fix_unicode_gaps(line.translate(no_punc)))\n",
    "        ngram_list.extend(list(ngrams(tokens, gram_size)))\n",
    "    ngram_dict = {}\n",
    "    for gram in ngram_list:\n",
    "        phrase = ' '.join(gram)\n",
    "        if phrase in ngram_dict:\n",
    "            ngram_dict[phrase] += 1\n",
    "        else:\n",
    "            ngram_dict[phrase] = 1\n",
    "    sorted_phrases = sorted(ngram_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(character.title())\n",
    "    for phrase in sorted_phrases[:5]:\n",
    "        joined_phrase = repair_nltk_contractions(phrase[0])\n",
    "        print(\"\\\"{}\\\" was said {} times\".format(joined_phrase, phrase[1]))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
